{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6c5d450b-b3ac-4fb3-b900-cdf075ba1c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete! Valid data saved in 'filtered_data_L3.xlsx' under 'valid_data' sheet.\n",
      "Abrupt peaks saved under 'abrupt_peaks' sheet.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the Excel file\n",
    "file_path = \"D:/IAQ_data/IAQ_Master_file/Cleaned_Duplicates/Without_Outliers/AbruptPeak.xlsx\"  # Change this to your actual file path\n",
    "df = pd.read_excel(file_path, sheet_name='L3')\n",
    "\n",
    "# Ensure 'date_time' column is in datetime format and sort by time\n",
    "df['date_time'] = pd.to_datetime(df['date_time'])\n",
    "df = df.sort_values(by='date_time')\n",
    "\n",
    "# Compute time difference in minutes\n",
    "df['time_diff'] = df['date_time'].diff().dt.total_seconds() / 60  # Convert seconds to minutes\n",
    "\n",
    "# Compute slope: Δpm10 / Δtime\n",
    "df['slope'] = df['pm10'].diff() / df['time_diff']\n",
    "\n",
    "# Calculate mean and standard deviation of slopes\n",
    "mean_slope = df['slope'].mean()\n",
    "std_slope = df['slope'].std()\n",
    "\n",
    "# Define threshold for abrupt peaks (3 standard deviations)\n",
    "threshold = mean_slope + (3 * std_slope)\n",
    "\n",
    "# Identify abrupt peaks\n",
    "df['abrupt_peak'] = df['slope'].abs() > threshold\n",
    "\n",
    "# Separate valid data and abrupt peaks\n",
    "valid_data = df[~df['abrupt_peak']].drop(columns=['time_diff', 'slope', 'abrupt_peak'])\n",
    "abrupt_peaks = df[df['abrupt_peak']].drop(columns=['time_diff', 'slope', 'abrupt_peak'])\n",
    "\n",
    "# Save to Excel with two sheets\n",
    "output_file = \"filtered_data_L3.xlsx\"\n",
    "with pd.ExcelWriter(output_file) as writer:\n",
    "    valid_data.to_excel(writer, sheet_name=\"valid_data\", index=False)\n",
    "    abrupt_peaks.to_excel(writer, sheet_name=\"abrupt_peaks\", index=False)\n",
    "\n",
    "print(f\"Processing complete! Valid data saved in '{output_file}' under 'valid_data' sheet.\")\n",
    "print(f\"Abrupt peaks saved under 'abrupt_peaks' sheet.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58b3c71-f709-4588-9865-3075bd250f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Hybrid method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "33feb6fb-00f1-4482-97df-bf9383205256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Processing complete!\n",
      "✔️ Valid data saved in 'filtered_data_hybrid_L113_C.xlsx' under 'valid_data' sheet.\n",
      "⚠️ Abrupt peaks saved under 'abrupt_peaks' sheet.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the Excel file\n",
    "file_path = \"D:/IAQ_data/IAQ_Master_file/Cleaned_Duplicates/Without_Outliers/AbruptPeak.xlsx\"   # Change to actual file path\n",
    "df = pd.read_excel(file_path, sheet_name='L113')\n",
    "\n",
    "# Ensure date_time is in datetime format and sort by time\n",
    "df['date_time'] = pd.to_datetime(df['date_time'])\n",
    "df = df.sort_values(by='date_time')\n",
    "\n",
    "# Compute time difference in minutes\n",
    "df['time_diff'] = df['date_time'].diff().dt.total_seconds() / 60  # Convert seconds to minutes\n",
    "\n",
    "# Compute slope: Δpm10 / Δtime\n",
    "df['slope'] = df['pm10'].diff() / df['time_diff']\n",
    "\n",
    "# Calculate mean and standard deviation of slopes\n",
    "mean_slope = df['slope'].mean()\n",
    "std_slope = df['slope'].std()\n",
    "\n",
    "# Calculate thresholds\n",
    "steep_threshold = mean_slope + (3 * std_slope)  # 3σ Rule\n",
    "percentile_threshold = df['slope'].quantile(0.98)  # 99th percentile\n",
    "\n",
    "# Identify abrupt peaks (if slope exceeds either threshold)\n",
    "df['abrupt_peak'] = (df['slope'].abs() > steep_threshold) | (df['slope'].abs() > percentile_threshold)\n",
    "\n",
    "# Short-duration peaks (must last ≤ 5 minutes)\n",
    "df['short_peak'] = df['time_diff'].shift(-1) <= 6\n",
    "\n",
    "# Sudden drop condition (next slope is a sharp decrease)\n",
    "df['sudden_drop'] = df['slope'].shift(-1) < (-steep_threshold)\n",
    "\n",
    "# Final abrupt peaks: Only if all three conditions are met\n",
    "df['final_peak'] = df['abrupt_peak'] & df['short_peak'] & df['sudden_drop']\n",
    "\n",
    "# Separate valid data and abrupt peaks\n",
    "valid_data = df[~df['final_peak']].drop(columns=['time_diff', 'slope', 'abrupt_peak', 'short_peak', 'sudden_drop', 'final_peak'])\n",
    "abrupt_peaks = df[df['final_peak']].drop(columns=['time_diff', 'slope', 'abrupt_peak', 'short_peak', 'sudden_drop', 'final_peak'])\n",
    "\n",
    "# Save to Excel with two sheets\n",
    "output_file = \"filtered_data_hybrid_L113_C.xlsx\"\n",
    "with pd.ExcelWriter(output_file) as writer:\n",
    "    valid_data.to_excel(writer, sheet_name=\"valid_data\", index=False)\n",
    "    abrupt_peaks.to_excel(writer, sheet_name=\"abrupt_peaks\", index=False)\n",
    "\n",
    "print(f\"✅ Processing complete!\")\n",
    "print(f\"✔️ Valid data saved in '{output_file}' under 'valid_data' sheet.\")\n",
    "print(f\"⚠️ Abrupt peaks saved under 'abrupt_peaks' sheet.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8efed9-d07c-4257-9bff-3fd9fbc7d04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hybrid method with rising points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2ec18f-603d-4d87-b314-e8724fe1aacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### making data per minute, if not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7643bdaf-a747-4391-bec6-d5bd98ef0b50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Processing complete!\n",
      "✔️ Valid data saved in 'filtered_data_hybridR_L3.xlsx' under 'valid_data' sheet.\n",
      "⚠️ Abrupt peaks (with rising points) saved under 'abrupt_peaks' sheet.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "file_path = \"D:/IAQ_data/IAQ_Master_file/Cleaned_Duplicates/Without_Outliers/AbruptPeak.xlsx\"  # Update with your file path\n",
    "df_raw = pd.read_excel(file_path, sheet_name='L113')\n",
    "\n",
    "\n",
    "# Convert date_time to datetime format and round to the nearest minute\n",
    "df_raw['date_time'] = pd.to_datetime(df_raw['date_time']).dt.floor('T')\n",
    "\n",
    "# Aggregate only pm10\n",
    "df = df_raw.groupby(['date_time', 'location_id'], as_index=False)['pm10'].mean()\n",
    "\n",
    "# Ensure date_time is in datetime format and sort by time\n",
    "df['date_time'] = pd.to_datetime(df['date_time'])\n",
    "df = df.sort_values(by='date_time')\n",
    "\n",
    "# Compute time difference in minutes\n",
    "df['time_diff'] = df['date_time'].diff().dt.total_seconds() / 60  # Convert seconds to minutes\n",
    "\n",
    "# Compute slope: Δpm10 / Δtime\n",
    "df['slope'] = df['pm10'].diff() / df['time_diff']\n",
    "\n",
    "# Calculate mean and standard deviation of slopes\n",
    "mean_slope = df['slope'].mean()\n",
    "std_slope = df['slope'].std()\n",
    "\n",
    "# Calculate thresholds\n",
    "steep_threshold = mean_slope + (3 * std_slope)  # 3σ Rule\n",
    "percentile_threshold = df['slope'].quantile(0.99)  # 98th percentile\n",
    "\n",
    "# Identify abrupt peaks (if slope exceeds either threshold)\n",
    "df['abrupt_peak'] = (df['slope'].abs() > steep_threshold) | (df['slope'].abs() > percentile_threshold)\n",
    "\n",
    "# Short-duration peaks (must last ≤ 5 minutes)\n",
    "df['short_peak'] = df['time_diff'].shift(-1) <= 5\n",
    "\n",
    "# Sudden drop condition (next slope is a sharp decrease)\n",
    "df['sudden_drop'] = df['slope'].shift(-1) < (-steep_threshold)\n",
    "\n",
    "# Identify rising points before an abrupt peak\n",
    "df['rising_point'] = (df['slope'] > steep_threshold) & (df['time_diff'] <= 6) & df['abrupt_peak'].shift(-1)\n",
    "\n",
    "# Final abrupt peaks: Include both detected peaks and rising points\n",
    "df['final_peak'] = df['abrupt_peak'] & df['short_peak'] & df['sudden_drop']\n",
    "df['final_peak'] = df['final_peak'] | df['rising_point']  # Include rising points\n",
    "\n",
    "# Separate valid data and abrupt peaks\n",
    "valid_data = df[~df['final_peak']].drop(columns=['time_diff', 'slope', 'abrupt_peak', 'short_peak', 'sudden_drop', 'rising_point', 'final_peak'])\n",
    "abrupt_peaks = df[df['final_peak']].drop(columns=['time_diff', 'slope', 'abrupt_peak', 'short_peak', 'sudden_drop', 'rising_point', 'final_peak'])\n",
    "\n",
    "# Save to Excel with two sheets\n",
    "output_file = \"filtered_data_hybridR_L.xlsx\"\n",
    "with pd.ExcelWriter(output_file) as writer:\n",
    "    valid_data.to_excel(writer, sheet_name=\"valid_data\", index=False)\n",
    "    abrupt_peaks.to_excel(writer, sheet_name=\"abrupt_peaks\", index=False)\n",
    "\n",
    "print(f\"✅ Processing complete!\")\n",
    "print(f\"✔️ Valid data saved in '{output_file}' under 'valid_data' sheet.\")\n",
    "print(f\"⚠️ Abrupt peaks (with rising points) saved under 'abrupt_peaks' sheet.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9aa8c0-aa79-41c9-8ac4-490f9f403855",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Now taking all datapoints from minute if data is converted from realtime per second data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a49139a9-667c-4a25-87ee-f8a1dcd2ceee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Processing complete!\n",
      "✔️ Valid data saved in 'filtered_data_hybridR_L3_persecond.xlsx' under 'valid_data' sheet.\n",
      "⚠️ Abrupt peaks (all raw data from those minutes) saved under 'abrupt_peaks' sheet.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "file_path = \"D:/IAQ_data/IAQ_Master_file/Cleaned_Duplicates/Without_Outliers/AbruptPeak.xlsx\"  # Update with your file path\n",
    "df_raw = pd.read_excel(file_path, sheet_name='L3')\n",
    "\n",
    "# Convert date_time to datetime format and round to the nearest minute\n",
    "df_raw['date_time'] = pd.to_datetime(df_raw['date_time']).dt.floor('T')\n",
    "\n",
    "# Aggregate only pm10\n",
    "df = df_raw.groupby(['date_time', 'location_id'], as_index=False)['pm10'].mean()\n",
    "\n",
    "# Ensure date_time is in datetime format and sort by time\n",
    "df['date_time'] = pd.to_datetime(df['date_time'])\n",
    "df = df.sort_values(by='date_time')\n",
    "\n",
    "# Compute time difference in minutes\n",
    "df['time_diff'] = df['date_time'].diff().dt.total_seconds() / 60  # Convert seconds to minutes\n",
    "\n",
    "# Compute slope: Δpm10 / Δtime\n",
    "df['slope'] = df['pm10'].diff() / df['time_diff']\n",
    "\n",
    "# Calculate mean and standard deviation of slopes\n",
    "mean_slope = df['slope'].mean()\n",
    "std_slope = df['slope'].std()\n",
    "\n",
    "# Calculate thresholds\n",
    "steep_threshold = mean_slope + (3 * std_slope)  # 3σ Rule\n",
    "percentile_threshold = df['slope'].quantile(0.99)  # 98th percentile\n",
    "\n",
    "# Identify abrupt peaks (if slope exceeds either threshold)\n",
    "df['abrupt_peak'] = (df['slope'].abs() > steep_threshold) | (df['slope'].abs() > percentile_threshold)\n",
    "\n",
    "# Short-duration peaks (must last ≤ 5 minutes)\n",
    "df['short_peak'] = df['time_diff'].shift(-1) <= 5\n",
    "\n",
    "# Sudden drop condition (next slope is a sharp decrease)\n",
    "df['sudden_drop'] = df['slope'].shift(-1) < (-steep_threshold)\n",
    "\n",
    "# Identify rising points before an abrupt peak\n",
    "df['rising_point'] = (df['slope'] > steep_threshold) & (df['time_diff'] <= 6) & df['abrupt_peak'].shift(-1)\n",
    "\n",
    "# Final abrupt peaks: Include both detected peaks and rising points\n",
    "df['final_peak'] = df['abrupt_peak'] & df['short_peak'] & df['sudden_drop']\n",
    "df['final_peak'] = df['final_peak'] | df['rising_point']  # Include rising points\n",
    "\n",
    "# Extract unique timestamps of abrupt peaks\n",
    "abrupt_peak_timestamps = df.loc[df['final_peak'], 'date_time'].dt.strftime('%Y-%m-%d %H:%M')\n",
    "\n",
    "# Filter raw data for all rows within detected abrupt peak minutes\n",
    "df_raw['minute_str'] = df_raw['date_time'].dt.strftime('%Y-%m-%d %H:%M')\n",
    "abrupt_peaks_raw = df_raw[df_raw['minute_str'].isin(abrupt_peak_timestamps)]\n",
    "\n",
    "# Drop auxiliary column\n",
    "df_raw.drop(columns=['minute_str'], inplace=True)\n",
    "\n",
    "# Separate valid data\n",
    "df_valid = df[~df['final_peak']].drop(columns=['time_diff', 'slope', 'abrupt_peak', 'short_peak', 'sudden_drop', 'rising_point', 'final_peak'])\n",
    "\n",
    "# Save to Excel with two sheets\n",
    "output_file = \"filtered_data_hybridR_L3_persecond.xlsx\"\n",
    "with pd.ExcelWriter(output_file) as writer:\n",
    "    df_valid.to_excel(writer, sheet_name=\"valid_data\", index=False)\n",
    "    abrupt_peaks_raw.to_excel(writer, sheet_name=\"abrupt_peaks\", index=False)\n",
    "\n",
    "print(f\"✅ Processing complete!\")\n",
    "print(f\"✔️ Valid data saved in '{output_file}' under 'valid_data' sheet.\")\n",
    "print(f\"⚠️ Abrupt peaks (all raw data from those minutes) saved under 'abrupt_peaks' sheet.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa81560f-b883-4d91-9706-52c237a46bc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f8d1ae-5dd6-4360-b413-20ccc47dc642",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Hybrid mwthod with background slope of 3 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ac88a24b-3383-4935-9e10-f0e3277d2105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Processing complete!\n",
      "✔️ Valid data saved in 'filtered_data_hybridR_L3_background.xlsx' under 'valid_data' sheet.\n",
      "⚠️ Abrupt peaks (all raw data from those minutes) saved under 'abrupt_peaks' sheet.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "file_path = \"D:/IAQ_data/IAQ_Master_file/Cleaned_Duplicates/Without_Outliers/AbruptPeak.xlsx\"  # Update with your file path\n",
    "df_raw = pd.read_excel(file_path, sheet_name='L3')\n",
    "\n",
    "# Convert date_time to datetime format and round to the nearest minute\n",
    "df_raw['date_time'] = pd.to_datetime(df_raw['date_time']).dt.floor('T')\n",
    "\n",
    "# Aggregate only pm10\n",
    "df = df_raw.groupby(['date_time', 'location_id'], as_index=False)['pm10'].mean()\n",
    "\n",
    "# Ensure date_time is in datetime format and sort by time\n",
    "df['date_time'] = pd.to_datetime(df['date_time'])\n",
    "df = df.sort_values(by='date_time')\n",
    "\n",
    "# Compute time difference in minutes\n",
    "df['time_diff'] = df['date_time'].diff().dt.total_seconds() / 60  # Convert seconds to minutes\n",
    "\n",
    "# Compute slope: Δpm10 / Δtime\n",
    "df['slope'] = df['pm10'].diff() / df['time_diff']\n",
    "\n",
    "# Function to compute background slope over the last 3 hours\n",
    "def compute_background_slope(df):\n",
    "    background_slopes = []\n",
    "    for i in range(len(df)):\n",
    "        start_time = df.loc[i, 'date_time'] - pd.Timedelta(hours=3)\n",
    "        subset = df[(df['date_time'] >= start_time) & (df['date_time'] <= df.loc[i, 'date_time'])]\n",
    "        if len(subset) < 2:  # If not enough data, look ahead\n",
    "            end_time = df.loc[i, 'date_time'] + pd.Timedelta(hours=(3 - (df.loc[i, 'date_time'] - df['date_time'].min()).total_seconds() / 3600))\n",
    "            subset = df[(df['date_time'] >= df.loc[i, 'date_time']) & (df['date_time'] <= end_time)]\n",
    "        background_slopes.append(subset['slope'].mean())\n",
    "    return np.array(background_slopes)\n",
    "\n",
    "# Compute background slope\n",
    "df['background_slope'] = compute_background_slope(df)\n",
    "\n",
    "# Calculate thresholds dynamically per data point\n",
    "steep_threshold = df['background_slope'] + (3 * df['slope'].std())\n",
    "percentile_threshold = df['slope'].rolling(window=180, min_periods=1).quantile(0.99)  # 3 hours ~ 180 minutes\n",
    "\n",
    "# Identify abrupt peaks (if slope exceeds either threshold)\n",
    "df['abrupt_peak'] = (df['slope'].abs() > steep_threshold) | (df['slope'].abs() > percentile_threshold)\n",
    "\n",
    "# Short-duration peaks (must last ≤ 5 minutes)\n",
    "df['short_peak'] = df['time_diff'].shift(-1) <= 5\n",
    "\n",
    "# Sudden drop condition (next slope is a sharp decrease)\n",
    "df['sudden_drop'] = df['slope'].shift(-1) < (-steep_threshold)\n",
    "\n",
    "# Identify rising points before an abrupt peak\n",
    "df['rising_point'] = (df['slope'] > steep_threshold) & (df['time_diff'] <= 6) & df['abrupt_peak'].shift(-1)\n",
    "\n",
    "# Final abrupt peaks: Include both detected peaks and rising points\n",
    "df['final_peak'] = df['abrupt_peak'] & df['short_peak'] & df['sudden_drop']\n",
    "df['final_peak'] = df['final_peak'] | df['rising_point']  # Include rising points\n",
    "\n",
    "# Extract unique timestamps of abrupt peaks\n",
    "abrupt_peak_timestamps = df.loc[df['final_peak'], 'date_time'].dt.strftime('%Y-%m-%d %H:%M')\n",
    "\n",
    "# Filter raw data for all rows within detected abrupt peak minutes\n",
    "df_raw['minute_str'] = df_raw['date_time'].dt.strftime('%Y-%m-%d %H:%M')\n",
    "abrupt_peaks_raw = df_raw[df_raw['minute_str'].isin(abrupt_peak_timestamps)]\n",
    "\n",
    "# Drop auxiliary column\n",
    "df_raw.drop(columns=['minute_str'], inplace=True)\n",
    "\n",
    "# Separate valid data\n",
    "df_valid = df[~df['final_peak']].drop(columns=['time_diff', 'slope', 'abrupt_peak', 'short_peak', 'sudden_drop', 'rising_point', 'final_peak', 'background_slope'])\n",
    "\n",
    "# Save to Excel with two sheets\n",
    "output_file = \"filtered_data_hybridR_L3_background.xlsx\"\n",
    "with pd.ExcelWriter(output_file) as writer:\n",
    "    df_valid.to_excel(writer, sheet_name=\"valid_data\", index=False)\n",
    "    abrupt_peaks_raw.to_excel(writer, sheet_name=\"abrupt_peaks\", index=False)\n",
    "\n",
    "print(f\"✅ Processing complete!\")\n",
    "print(f\"✔️ Valid data saved in '{output_file}' under 'valid_data' sheet.\")\n",
    "print(f\"⚠️ Abrupt peaks (all raw data from those minutes) saved under 'abrupt_peaks' sheet.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffee2d8-5c05-43c4-b133-6da93f82383c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d5f5b9-4de2-4723-910c-cc1e73268b02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
