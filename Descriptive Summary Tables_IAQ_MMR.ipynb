{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd666e7f",
   "metadata": {},
   "source": [
    "# The following code provides a structured analysis of the dataset (over 0.65 million dataponts of premonsoon 2024), making it ready for presentation in a research article.\n",
    "It includes:\n",
    "\n",
    "1. Descriptive Summary – Generates a summary table and calculates the mean PM10 concentration for each location.\n",
    "2. Category-wise Analysis – Computes statistics for different categories of locations.\n",
    "3. Sub-Category-wise Analysis – Computes statistics for different sub-categories under categories of locations.\n",
    "4.Regional Analysis – Breaks down air quality data by region codes and categories.\n",
    "5.Regional Analysis – Breaks down air quality data by region codes (municipality) only.\n",
    "\n",
    "It presents as mean ± S.D.; median; min-max"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe9e5b7-461a-4b68-baac-4bb46fb4bfb9",
   "metadata": {},
   "source": [
    "#### 1. Summary of whole data\n",
    "##### Output as summary table and data preparation of mean of each location (data_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efb9fd36-e1d5-48f0-8791-bb6f8e88a1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def calculate_pm10_summary(input_file, min_max_file, output_file, location_mean_output):\n",
    "   
    "    df = pd.read_excel(input_file)\n",
    "    min_max_df = pd.read_excel(min_max_file) # data_1\n",
    "    \n",
    "    # Ensure the necessary columns exist\n",
    "    required_columns = {'location_id', 'hourly_pm10', 'category', 'sub_category', 'criteria', 'region_code'}\n",
    "    if not required_columns.issubset(df.columns):\n",
    "        raise ValueError(f\"Missing required columns: {required_columns - set(df.columns)}\")\n",
    "    \n",
    "    if 'pm10' not in min_max_df.columns:\n",
    "        raise ValueError(\"Missing required column: 'pm10' in min_max_file\")\n",
    "    \n",
    "    # mean PM10 for each location_id\n",
    "    location_means = df.groupby('location_id', as_index=False)['hourly_pm10'].mean() # data_3 created- mean is calculated for each location_id (data_2)\n",
    "    location_means.columns = ['location_id', 'mean_location_pm10']\n",
    "    \n",
    "    # Merge with categorical columns to retain additional information\n",
    "    location_means = location_means.merge(df[['location_id', 'category', 'sub_category', 'criteria', 'region_code','lat','lon']].drop_duplicates(), on='location_id', how='left')\n",
    "    \n",
    " 
    "    overall_mean = location_means['mean_location_pm10'].mean() # final mean is calculated from each locations mean\n",
    "    overall_std = location_means['mean_location_pm10'].std()   # final std.dev is calculated from dataset of each locations mean.\n",
    "    overall_median = location_means['mean_location_pm10'].median() # fina median is calculated from dataset of each locations mean.\n",
    "    \n",
    "    # Compute min and max from the second dataset\n",
    "    overall_min = min_max_df['pm10'].min() # data_1\n",
    "    overall_max = min_max_df['pm10'].max()\n",
    "    \n",
    "    # Prepare summary table\n",
    "    summary_data = {\n",
    "        \"Metric\": [\"Mean ± std. deviation\", \"Median\", \"Min-Max\"],\n",
    "        \"Concentration\": [f\"{overall_mean:.2f} ± {overall_std:.2f}\",\n",
    "                           f\"{overall_median:.2f}\",\n",
    "                           f\"{overall_min:.2f}-{overall_max:.2f}\"]\n",
    "    }\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    \n",
    "  
    "    summary_df.to_excel(output_file, index=False)\n",
    "    location_means.to_excel(location_mean_output, index=False)\n",
    "    print(f\"Summary saved to {output_file}\")\n",
    "    print(f\"Location means saved to {location_mean_output}\")\n",
    "\n",
    "# Example usage\n",
    "# calculate_pm10_summary('data_2.xlsx', 'data_1.xlsx', 'summary_output.xlsx', 'location_means.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf4ba0c4-7be2-49b5-ae81-0634336031a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary saved to summary_output.xlsx\n",
      "Location means saved to location_means.xlsx\n"
     ]
    }
   ],
   "source": [
    "calculate_pm10_summary('C:/Users/Atique/Spatial_Research_Coding_Post/HourlyAverage_Data/hourly_pm10_output_location_wise.xlsx', 'D:/IAQ_data/IAQ_Master_file/Without_outlier/Percentile_Outlier/Master_PM10_Percentile.xlsx','summary_output.xlsx', 'location_means.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc76554-250d-4de0-9ce9-5763a1ecdc7c",
   "metadata": {},
   "source": [
    "##### 2. Acc to category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e122ac2f-95cd-4654-bb71-e164321e6a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "data_2 = pd.read_excel(\"C:/Users/Atique/Spatial_Research_Coding_Post/HourlyAverage_Data/hourly_pm10_output_location_wise.xlsx\")  # Contains 'hourly_pm10'\n",
    "data_1 = pd.read_excel(\"D:/IAQ_data/IAQ_Master_file/Without_outlier/Percentile_Outlier/Master_PM10_Percentile.xlsx\")  # Contains 'pm10'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "64c0e7d7-ad82-4bf1-8e32-2bc60fd6fa43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary tables saved as 'summary_tables.xlsx'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#  Load the datasets\n",
    "# data_2 = pd.read_excel(\"C:/Users/Atique/Spatial_Research_Coding_Post/HourlyAverage_Data/hourly_pm10_output_location_wise.xlsx\")  # Contains 'hourly_pm10'\n",
    "# data_1 = pd.read_excel(\"D:/IAQ_data/IAQ_Master_file/Without_outlier/Percentile_Outlier/Master_PM10_Percentile.xlsx\")  # Contains 'pm10'\n",
    "\n",
    "\n",
    "# Consider only required columns\n",
    "cols = [\"location_id\", \"category\", \"sub_category\", \"criteria\", \"region_code\"]\n",
    "data_2 = data_2[[\"hour\", \"hourly_pm10\"] + cols]\n",
    "data_1 = data_1[[\"pm10\"] + cols]\n",
    "\n",
    "# Compute mean for each unique location_id first\n",
    "data_2_avg = data_2.groupby(\"location_id\")[\"hourly_pm10\"].mean().reset_index() # i.e data_3 created\n",
    "data_1_min_max = data_1.groupby(\"location_id\")[\"pm10\"].agg([\"min\", \"max\"]).reset_index()\n",
    "\n",
    "# Merge back to get category-level statistics\n",
    "data_2_merged = data_2_avg.merge(data_2[cols].drop_duplicates(), on=\"location_id\", how=\"left\")\n",
    "data_1_merged = data_1_min_max.merge(data_1[cols].drop_duplicates(), on=\"location_id\", how=\"left\")\n",
    "\n",
    "# Dictionary to store summary tables\n",
    "summary_tables = {}\n",
    "\n",
    "# Iterate over each unique category\n",
    "for category in data_2_merged[\"category\"].unique():\n",
    "    df_cat2 = data_2_merged[data_2_merged[\"category\"] == category]  # Mean hourly PM10\n",
    "    df_cat1 = data_1_merged[data_1_merged[\"category\"] == category]  # Min & Max PM10\n",
    "\n",
    " 
    "    mean_val = df_cat2[\"hourly_pm10\"].mean()\n",
    "    std_val = df_cat2[\"hourly_pm10\"].std()\n",
    "    median_val = df_cat2[\"hourly_pm10\"].median()\n",
    "    min_val = df_cat1[\"min\"].min()\n",
    "    max_val = df_cat1[\"max\"].max()\n",
    "\n",
    "  
    "    summary_df = pd.DataFrame({\n",
    "        \"Metric\": [\"Mean ± std. deviation\", \"Median\", \"Min-Max\"],\n",
    "        \"Concentration\": [f\"{mean_val:.2f} ± {std_val:.2f}\", f\"{median_val:.2f}\", f\"{min_val:.2f} - {max_val:.2f}\"]\n",
    "    })\n",
    "    \n",
    "    summary_tables[category] = summary_df\n",
    "\n",
    
    "with pd.ExcelWriter(\"summary_tables.xlsx\") as writer:\n",
    "    for category, df in summary_tables.items():\n",
    "        df.to_excel(writer, sheet_name=category, index=False)\n",
    "\n",
    "print(\"Summary tables saved as 'summary_tables.xlsx'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fbedac-8427-4ee9-b3e8-dd210c308330",
   "metadata": {},
   "source": [
    "#### 3. Acc to sub_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "509fb393-c886-4f5a-a56a-4a3050b05975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary table saved as 'summary_tables.xlsx'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "# data_2 = pd.read_excel(\"C:/Users/Atique/Spatial_Research_Coding_Post/HourlyAverage_Data/hourly_pm10_output_location_wise.xlsx\")  # Contains 'hourly_pm10'\n",
    "# data_1 = pd.read_excel(\"D:/IAQ_data/IAQ_Master_file/Without_outlier/Percentile_Outlier/Master_PM10_Percentile.xlsx\") \n",
    "\n",
    "# Consider only required columns\n",
    "cols = [\"location_id\", \"category\", \"sub_category\", \"criteria\", \"region_code\"]\n",
    "data_2 = data_2[[\"hour\", \"hourly_pm10\"] + cols]\n",
    "data_1 = data_1[[\"pm10\"] + cols]\n",
    "\n",
    "# Compute mean for each unique location_id first\n",
    "data_2_avg = data_2.groupby(\"location_id\")[\"hourly_pm10\"].mean().reset_index()\n",
    "data_1_min_max = data_1.groupby(\"location_id\")[\"pm10\"].agg([\"min\", \"max\"]).reset_index()\n",
    "\n",
    "# Merge back to get category and sub_category-level statistics\n",
    "data_2_merged = data_2_avg.merge(data_2[cols].drop_duplicates(), on=\"location_id\", how=\"left\")\n",
    "data_1_merged = data_1_min_max.merge(data_1[cols].drop_duplicates(), on=\"location_id\", how=\"left\")\n",
    "\n",
    "# List to store summary data\n",
    "summary_list = []\n",
    "\n",
    "# Iterate over unique categories and sub_categories\n",
    "for (category, sub_category), df_cat1 in data_2_merged.groupby([\"category\", \"sub_category\"]):\n",
    "    df_cat2 = data_1_merged[(data_1_merged[\"category\"] == category) & (data_1_merged[\"sub_category\"] == sub_category)]\n",
    "    \n",
    "    mean_val = df_cat1[\"hourly_pm10\"].mean()\n",
    "    std_val = df_cat1[\"hourly_pm10\"].std()\n",
    "    median_val = df_cat1[\"hourly_pm10\"].median()\n",
    "    min_val = df_cat2[\"min\"].min()\n",
    "    max_val = df_cat2[\"max\"].max()\n",
    "    \n",
    "    summary_list.append([category, sub_category, \"Mean ± std. deviation\", f\"{mean_val:.2f} ± {std_val:.2f}\"])\n",
    "    summary_list.append([category, sub_category, \"Median\", f\"{median_val:.2f}\"])\n",
    "    summary_list.append([category, sub_category, \"Min-Max\", f\"{min_val:.2f} - {max_val:.2f}\"])\n",
    "\n",
    "# Convert list to DataFrame\n",
    "summary_df = pd.DataFrame(summary_list, columns=[\"Category\", \"Sub_Category\", \"Metric\", \"Concentration\"])\n",
    "\n",
    "# Save to Excel in a single sheet\n",
    "summary_df.to_excel(\"summary_tables.xlsx\", index=False)\n",
    "\n",
    "print(\"Summary table saved as 'summary_tables.xlsx'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95770bf5-2e32-4a9b-9da0-087aaf8716fe",
   "metadata": {},
   "source": [
    "#### 4. Acc to Region_code and category of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ee8645b5-a103-4059-b203-6c3e583e98ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary table saved as 'summary_tables.xlsx'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# # Load the datasets\n",
    "# data_2 = pd.read_excel(\"C:/Users/Atique/Spatial_Research_Coding_Post/HourlyAverage_Data/hourly_pm10_output_location_wise.xlsx\")  # Contains 'hourly_pm10'\n",
    "# data_1 = pd.read_excel(\"D:/IAQ_data/IAQ_Master_file/Without_outlier/Percentile_Outlier/Master_PM10_Percentile.xlsx\") \n",
    "\n",
    "# Consider only required columns\n",
    "cols = [\"location_id\", \"category\", \"sub_category\", \"criteria\", \"region_code\"]\n",
    "data_2 = data_2[[\"hour\", \"hourly_pm10\"] + cols]\n",
    "data_1 = data_1[[\"pm10\"] + cols]\n",
    "\n",
    "# Compute mean for each unique location_id first\n",
    "data_2_avg = data_2.groupby(\"location_id\")[\"hourly_pm10\"].mean().reset_index()\n",
    "data_1_min_max = data_1.groupby(\"location_id\")[\"pm10\"].agg([\"min\", \"max\"]).reset_index()\n",
    "\n",
    "# Merge back to get region_code and category-level statistics\n",
    "data_2_merged = data_2_avg.merge(data_2[cols].drop_duplicates(), on=\"location_id\", how=\"left\")\n",
    "data_1_merged = data_1_min_max.merge(data_1[cols].drop_duplicates(), on=\"location_id\", how=\"left\")\n",
    "\n",
    "# List to store summary data\n",
    "summary_list = []\n",
    "\n",
    "# Iterate over unique region_code and category\n",
    "for (region_code, category), df_cat1 in data_2_merged.groupby([\"region_code\", \"category\"]):\n",
    "    df_cat2 = data_1_merged[(data_1_merged[\"region_code\"] == region_code) & (data_1_merged[\"category\"] == category)]\n",
    "    \n",
    "    mean_val = df_cat1[\"hourly_pm10\"].mean()\n",
    "    std_val = df_cat1[\"hourly_pm10\"].std()\n",
    "    median_val = df_cat1[\"hourly_pm10\"].median()\n",
    "    min_val = df_cat2[\"min\"].min()\n",
    "    max_val = df_cat2[\"max\"].max()\n",
    "    \n",
    "    summary_list.append([region_code, category, \"Mean ± std. deviation\", f\"{mean_val:.2f} ± {std_val:.2f}\"])\n",
    "    summary_list.append([region_code, category, \"Median\", f\"{median_val:.2f}\"])\n",
    "    summary_list.append([region_code, category, \"Min-Max\", f\"{min_val:.2f} - {max_val:.2f}\"])\n",
    "\n",
    "# Convert list to DataFrame\n",
    "summary_df = pd.DataFrame(summary_list, columns=[\"Region_Code\", \"Category\", \"Metric\", \"Concentration\"])\n",
    "\n",
    "# Save to Excel in a single sheet\n",
    "summary_df.to_excel(\"summary_tables.xlsx\", index=False)\n",
    "\n",
    "print(\"Summary table saved as 'summary_tables.xlsx'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b83c801-b436-41e1-a7f7-b3efd93203a4",
   "metadata": {},
   "source": [
    "#### 5. Acc to Region code only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d7dbcd29-d390-45fa-b933-9cfdcf208dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary table saved as 'summary_tables.xlsx'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "# data_2 = pd.read_excel(\"C:/Users/Atique/Spatial_Research_Coding_Post/HourlyAverage_Data/hourly_pm10_output_location_wise.xlsx\")  # Contains 'hourly_pm10'\n",
    "# data_1 = pd.read_excel(\"D:/IAQ_data/IAQ_Master_file/Without_outlier/Percentile_Outlier/Master_PM10_Percentile.xlsx\") \n",
    "\n",
    "# Consider only required columns\n",
    "cols = [\"location_id\", \"region_code\"]\n",
    "data_2 = data_2[cols + [\"hourly_pm10\"]]\n",
    "data_1 = data_1[cols + [\"pm10\"]]\n",
    "\n",
    "# Compute mean for each unique location_id first\n",
    "data_2_avg = data_2.groupby(\"location_id\")[\"hourly_pm10\"].mean().reset_index()\n",
    "data_1_min_max = data_1.groupby(\"location_id\")[\"pm10\"].agg([\"min\", \"max\"]).reset_index()\n",
    "\n",
    "# Merge back to get region_code-level statistics\n",
    "data_2_merged = data_2_avg.merge(data_2[cols].drop_duplicates(), on=\"location_id\", how=\"left\")\n",
    "data_1_merged = data_1_min_max.merge(data_1[cols].drop_duplicates(), on=\"location_id\", how=\"left\")\n",
    "\n",
    "# List to store summary data\n",
    "summary_list = []\n",
    "\n",
    "# Iterate over unique region_code\n",
    "for region_code, df_region1 in data_2_merged.groupby(\"region_code\"):\n",
    "    df_region2 = data_1_merged[data_1_merged[\"region_code\"] == region_code]\n",
    "    \n",
    "    mean_val = df_region1[\"hourly_pm10\"].mean()\n",
    "    std_val = df_region1[\"hourly_pm10\"].std()\n",
    "    median_val = df_region1[\"hourly_pm10\"].median()\n",
    "    min_val = df_region2[\"min\"].min()\n",
    "    max_val = df_region2[\"max\"].max()\n",
    "    \n",
    "    summary_list.append([region_code, \"Mean ± std. deviation\", f\"{mean_val:.2f} ± {std_val:.2f}\"])\n",
    "    summary_list.append([region_code, \"Median\", f\"{median_val:.2f}\"])\n",
    "    summary_list.append([region_code, \"Min-Max\", f\"{min_val:.2f} - {max_val:.2f}\"])\n",
    "\n",
    "# Convert list to DataFrame\n",
    "summary_df = pd.DataFrame(summary_list, columns=[\"Region_Code\", \"Metric\", \"Concentration\"])\n",
    "\n",
    "# Save to Excel in a single sheet\n",
    "summary_df.to_excel(\"summary_tables.xlsx\", index=False)\n",
    "\n",
    "print(\"Summary table saved as 'summary_tables.xlsx'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5621c7-09f0-448c-924a-d138e34a4fa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3696b4f-3bbb-452d-88fb-d8d4a8b197cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb56897-ed4f-4a10-a2c3-edeeac791ddd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2093e69-a57b-415b-b37f-b6bb43dd1042",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
